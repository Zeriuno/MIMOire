Définition de la résilience

La résilience est une propriété inhérente à la structure chimio-physique d'un matérieau ou bien à la structure d'un système.
-définition du SI-
Illusion de la maîtrise des systèmes complexes (HOLLNAGEL, PARIES...)


Capacité de gérer l'erreur et de le contenir à l'intérieur d'un cadre d'équilibre stable.
-> résilience dans la vision géographique
Un système avec une plus grande complexité est moins facile à modeliser. L'analyse des cas d'échec est donc à aborder dans cette optique, comme moyen pour découvrir des parties du système.

## Ce qui n'est pas résilience

Elle ne peut pas être définie de manière contractuelle.
Lors des contrats de préstation de services, il est désormais incontournable que les deux parties s'accordent sur quels sont les paramètres clés que le service doit respecter (SLA).
Ainsi un service d'hébergement pourra s'engager à rendre les serveurs loués par son client accessibles 23 heures et 50 minutes par jour. La transaction commerciale se structure ainsi autour non seulement d'un service principal qui est rendu, mais aussi de la prise en charge des cas d'échec de celui-ci. L'obbligation stipulée est dans ce cas une obligation de résultats et non de moyens.
Ce genre d'accord permet à une DSI de structurer son fonctionnement autour d'une hypothèse consacrée juridiquement. Cette hypothèse n'est pas pour autant garantie


##
Outils: éviter les goulots d'étranglement et les SPOF. Cela a une application dans la partie technique du SI, tout comme dans celle humaine (dans laquelle les SPOF sont représentés par la repartition des connaissances -approche agile- ainsi que par la hiérarchie)


# Conclusion

Critique de la résilience
[la résilience: après tout, pourquoi? L'idée qu'il soit important de concevoir des systèmes qui fonctionnent apparaît tellement de bon sens qu'il serait possible de mener toute une enquête à ce sujet sans jamais la questionner. C'est d'ailleurs ce que nous venons de faire. ]

un système informatique résilient vise à rendre l'humain superflu. Complètement résilient, une fois programmé, capable de se réparer de lui-même et d'augmenter ses capacités de réponse en fonction des évènements passés via une fonction d'apprentissage, celui-ci est parfaitement autonome. Dans une situation paradoxale, est uniquement l'incapacité des humains à réaliser un tel système qui les rends nécessaires et indispensables: leur utilité est une conséquence directe de leurs erreurs; tandis qu'on essaie de les éliminer du système des machines, les failles des humains sont l'élément qui continue à garantir leur place.

Au delà d'une curiosité intellectuelle, le paradoxe se révèle encore une fois une ressource de la pensée très profonde. Loin d'être un simple sophisme, celui-ci révèle les nœuds ontologiques que les apparences ne peuvent pas saisir: si, poussés à programmer leur futilité, les humains se découvrent utiles simplement grâce à leurs erreurs, cette équation révèle l'inumanité d'une telle ambition et dénonce, par conséquent, un danger. Le risque est, une fois de plus, que dans le cadre d'une telle recherche, qui, si elle est en accord avec les aspirations, elle se révèle néanmoins être fondamentalement contraire aux principes de l'humain, les conséquences soient bien plus profondes qu'on ne sait l'anticiper: les erreurs propres à l'hommes rentrent encore une fois en jeux et, après avoir suscité la recherche de la résilience, invitent à l'approcher avec vigilance.

[liaison à parfaire]

Cette critique a d'ailleurs déjà été formulée, à un moment où les outils informatiques étaient encore dans un état primitif. Dans un contexte intellectuel qui ne s'interdisait pas de critiquer la validité des choix pris par la hiérarchie et des réponses automatiques, *Fail Safe* et une histoire qui explore les conséquences d'une procédure conçue pour être résiliente et pleinement automatisée qui, de ce fait, va déclencher un carnage thermonucléaire.
Ecrit par Eugene Burdick et Harvey Wheeler en 1962, ce roman a deux années plus tard été transposé au cinéma par Sidney Lumet. Reproposée au public par Stephen Frears en 2000, sous les aspects d'un vieux noir et blanc et l'inactuel contexte de la guerre froide l'histoire cache le thème fondamental de la possilité de débrancher les machines et arrêter les processus.

À l'heure de la surveillance planetaire, de gouvernance algorithmique, de l'intelligence artificielle et des réseaux néuronaux profonds, des assassinats confiés aux drones, des failles informatiques qui ciblent les centres re recherche nucléaire, la mise en place de systèmes résilients impose une nouvelle approche. Les femmes et les hommes impliqués dans de tels projets se doivent d'en évaluer la retombée éthique, de prendre en compte la possibilité de faire objection de conscience et ne pas les mettre en oeuvre. Il est désormais impératif de relever un autre défi et de considérer que la mise en place de systèmes capables de contourner l'erreur et assurer un fonctionnement virtuellement pérenne peut être une faute.
